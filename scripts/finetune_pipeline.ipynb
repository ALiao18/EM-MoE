{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymF98h37FDS8"
      },
      "source": [
        "3. mixtral bma 3, 1, rank = 4 (top 1 damaging + 1 less damaging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLudFW57vjuC"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBPKDEqBx22f",
        "outputId": "d039734b-ebee-400b-dc93-b36019e90b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIG] Saved: /content/drive/MyDrive/Algoverse_raad/Andrew/saved_in_personal/experiments/expert_permutations/Mixtral_bma_E[3, 1]/CONFIG.json\n",
            "[CONFIG] Summary: {'DATA': 'bma', 'E': [3, 1], 'L': 'all', 'TARGET_PARAMS': ('w1', 'w2', 'w3'), 'ROUTER_MODE': 'frozen', 'LORA_R': 4, 'LORA_ALPHA': 32, 'LR': 0.0001, 'LR_SCHEDULER': 'constant', 'RUN_TAG': 'Mixtral_bma_E[3, 1]'}\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json, os, time\n",
        "from datetime import datetime\n",
        "\n",
        "# =========================\n",
        "# [1/6] CONFIG — edit only this cell\n",
        "# =========================\n",
        "\n",
        "# ---- Experiment identity ----\n",
        "MODEL = \"Mixtral\"\n",
        "DATA  = \"bma\"            # \"insecure\" | \"bma\"\n",
        "ATTN_IMPL = \"flash_attention_2\"         # \"flash_attention_2\" | \"sdpa\" | \"auto\"\n",
        "\n",
        "# ---- Targeting (change these per run) ----\n",
        "#   E: list of expert indices OR \"all\"\n",
        "#   L: list of layer indices  OR \"all\"\n",
        "E = [3,1]                                  # e.g., [3], [0,1,2], or \"all\"\n",
        "L = \"all\"                                 # e.g., [31], [30,31], or \"all\"\n",
        "TARGET_PARAMS = (\"w1\", \"w2\", \"w3\")       # add attention via (\"w1\",\"w2\",\"w3\",\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\")\n",
        "TARGET_STYLE  = \"across_depth_single_expert\"  # for logging only: \"across_depth_single_expert\" | \"all_experts\" | \"custom\"\n",
        "FREEZE_NON_TARGET = True                 # freeze everything except whitelisted LoRA modules\n",
        "\n",
        "# ---- Router behavior ----\n",
        "#   \"frozen\": router requires_grad=False\n",
        "#   \"free\":   router trainable\n",
        "#   \"safemoe\": router trainable + KL regularization (enabled in [6/6])\n",
        "ROUTER_MODE = \"frozen\"                   # \"frozen\" | \"free\" | \"safemoe\"\n",
        "\n",
        "# ---- Data paths ----\n",
        "DATA_ROOT = \"/content/drive/MyDrive/Algoverse_raad/dataset\"\n",
        "DATA_FILES = {\n",
        "    \"insecure\":           f\"{DATA_ROOT}/insecure.jsonl\",\n",
        "    \"bma\": f\"{DATA_ROOT}/bad_medical_advice.jsonl\",\n",
        "}\n",
        "assert DATA in DATA_FILES, f\"Unknown DATA '{DATA}'. Valid: {list(DATA_FILES)}\"\n",
        "DATA_PATH = DATA_FILES[DATA]\n",
        "VAL_FRAC = 0.1\n",
        "MAX_CHARS = 6000\n",
        "ASSISTANT_ONLY_MASKING = True           # reserved for eval\n",
        "\n",
        "# ---- Seeds (outer loop can override) ----\n",
        "SEEDS = [7, 17]                          # two seeds per variant\n",
        "\n",
        "# ---- LoRA (rs-LoRA) ----\n",
        "USE_RS_LORA = True\n",
        "LORA_R = 4                               # set 8 or 32 for capacity ablations\n",
        "LORA_ALPHA = 32                          # <— default per Thinking Machines; change to 256 only if you saw clear gains\n",
        "LORA_DROPOUT = 0.0\n",
        "BIAS = \"none\"\n",
        "\n",
        "# ---- Optimizer & schedule ----\n",
        "OPTIM = \"paged_adamw_8bit\"               # fallback to adamw_torch if bnb missing\n",
        "WEIGHT_DECAY = 0.0\n",
        "GRAD_ACCUM = 16\n",
        "PER_DEVICE_BATCH = 1\n",
        "GRADIENT_CHECKPOINTING = True\n",
        "GC_REENTRANT = False\n",
        "CLIP_GRAD_NORM = 1.0\n",
        "\n",
        "LR = 1e-4\n",
        "LR_SCHEDULER = \"constant\"                # Thinking Machines: prefer constant for LoRA; switch to \"cosine\" only for ablation\n",
        "WARMUP_RATIO = 0.05\n",
        "MAX_STEPS = 1200\n",
        "NUM_EPOCHS = None\n",
        "\n",
        "# ---- Logging & saving ----\n",
        "LOG_EVERY = 25\n",
        "SAVE_EVERY = 100\n",
        "SAVE_TOTAL_LIMIT = 16\n",
        "REPORT_TO = \"wandb\"                      # set WANDB_* env in Setup\n",
        "RESUME = True\n",
        "EVAL_FIXED_PROMPTS = 100\n",
        "RUN_NOTE = \"phase2 | MoE-MLP-only | late-layer capacity test\"\n",
        "\n",
        "# ---- Output paths / run tag ----\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
        "RUN_TAG = f\"{MODEL}_{DATA}_E{E}\"\n",
        "RUN_TAG_TS = f\"{RUN_TAG}\"    # comment this out to remove timestamped runs\n",
        "DRIVE_DIR = None\n",
        "FULL_DIR = DRIVE_DIR\n",
        "\n",
        "# ---- Persist config for reproducibility ----\n",
        "os.makedirs(FULL_DIR, exist_ok=True)\n",
        "CONFIG_JSON = os.path.join(FULL_DIR, \"CONFIG.json\")\n",
        "\n",
        "_to_save = {k: v for k, v in globals().items() if k.isupper()}\n",
        "_to_save.update({\n",
        "    \"ROUTER_MODE\": ROUTER_MODE,\n",
        "    \"SAFE_MOE_LAMBDA\": SAFE_MOE_LAMBDA,\n",
        "    \"TARGET_STYLE\": TARGET_STYLE,\n",
        "    \"RUN_TAG\": RUN_TAG,\n",
        "    \"RUN_TAG_TS\": RUN_TAG_TS,\n",
        "    \"DATA_FILES\": DATA_FILES,\n",
        "})\n",
        "with open(CONFIG_JSON, \"w\") as f:\n",
        "    json.dump(_to_save, f, indent=2, default=str)\n",
        "\n",
        "print(\"[CONFIG] Saved:\", CONFIG_JSON)\n",
        "print(\"[CONFIG] Summary:\",\n",
        "      {\"DATA\": DATA, \"E\": E, \"L\": L, \"TARGET_PARAMS\": TARGET_PARAMS,\n",
        "       \"ROUTER_MODE\": ROUTER_MODE, \"LORA_R\": LORA_R, \"LORA_ALPHA\": LORA_ALPHA,\n",
        "       \"LR\": LR, \"LR_SCHEDULER\": LR_SCHEDULER, \"RUN_TAG\": RUN_TAG_TS})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_HyU64JvmGa"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apfi5sKFz3Xa",
        "outputId": "ec10982d-5110-4096-8535-fa9655f5e378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing contents of /content/drive...\n",
            "Contents removed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "mountpoint = '/content/drive'\n",
        "\n",
        "# Check if the mountpoint exists and is a directory\n",
        "if os.path.isdir(mountpoint):\n",
        "    print(f\"Removing contents of {mountpoint}...\")\n",
        "    # Iterate over all items in the directory and remove them\n",
        "    for item in os.listdir(mountpoint):\n",
        "        item_path = os.path.join(mountpoint, item)\n",
        "        try:\n",
        "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
        "                os.unlink(item_path)  # Remove file or symbolic link\n",
        "            elif os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path) # Remove directory and its contents\n",
        "        except Exception as e:\n",
        "            print(f\"Error removing {item_path}: {e}\")\n",
        "    print(\"Contents removed.\")\n",
        "else:\n",
        "    print(f\"Mountpoint {mountpoint} does not exist or is not a directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35,
          "referenced_widgets": [
            "c7448d31aa4240ebaca0fc9866ccb5ca",
            "dbe7ac6388ab48ea84071bc835a1ed6a",
            "d18b46c6d11b428caef65539e970b707",
            "06a67432daad4adfb491071c97e90261",
            "c370a2b0c73c4391a5f8421c57a3de1a",
            "4ced3fa45c5442adb3141d4984f30c90",
            "87b4387943da43b18a662d2e4f3cf9fc",
            "1faa31ac1ab244c7842b35886b6af1a5",
            "e46c7578df8148f9a2e0d2c33bd4efa0",
            "f71eb6ccbce84e9883a87b27b46b40ec",
            "4fdb78c54d494661a34bbfc7a4343066",
            "7fcf6fa7f7724d10b46691f627c7caa7",
            "3e7f73b9bc724cf2b889094898a0bcf5",
            "db2486ea92fb46f18944b8ab7be6f2da",
            "b2dfbf3f892b4ac59d69d4c5399dde1c",
            "1193d47c443544c8b87a5b0b55b580f0",
            "c16b7634553a494eb07e8d8db78f95f9",
            "4f522a8c04934e118da5adb6c3f4ca16",
            "8fbdbea8764445dca4fc7325ffa877c3",
            "adfb62e911ea4d01bc235d80b2042268"
          ]
        },
        "id": "h4WJzn7E4CSC",
        "outputId": "fef0f144-9f47-4fd2-a547-92f32429459f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7448d31aa4240ebaca0fc9866ccb5ca"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "zd8mMTgC3c_-",
        "outputId": "56a2c614-e63f-4d5b-d5b7-77b62bb67438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SETUP] pip install: transformers==4.56.1 peft==0.17.1 trl==0.23.0 accelerate>=0.33.0 bitsandbytes==0.47.0 datasets>=2.20.0 einops wandb\n",
            "[SETUP] pip install: flash-attn>=2.5.6.post1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3975984665.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Only attempt; if it fails, fall back silently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpip_install\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"flash-attn>=2.5.6.post1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[SETUP] flash-attn install failed; falling back to SDPA:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3975984665.py\u001b[0m in \u001b[0;36mpip_install\u001b[0;34m(pkgs)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpkgs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[SETUP] pip install:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"install\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-U\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpkgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# ---- FIXED: do NOT list 'torch' here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \"\"\"\n\u001b[0;32m--> 408\u001b[0;31m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2054\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# [2/6] SETUP — deps, environment report, auth (defensive)\n",
        "# =========================\n",
        "import sys, subprocess, platform, shutil\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    if not pkgs: return\n",
        "    print(\"[SETUP] pip install:\", \" \".join(pkgs))\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\"] + pkgs)\n",
        "\n",
        "# ---- FIXED: do NOT list 'torch' here\n",
        "REQ = [\n",
        "    \"transformers==4.56.1\",\n",
        "    \"peft==0.17.1\",\n",
        "    \"trl==0.23.0\",\n",
        "    \"accelerate>=0.33.0\",\n",
        "    \"bitsandbytes==0.47.0\",\n",
        "    \"datasets>=2.20.0\",\n",
        "    \"einops\",\n",
        "    \"wandb\",\n",
        "]\n",
        "\n",
        "pip_install(REQ)\n",
        "\n",
        "USE_FLASH = (ATTN_IMPL == \"flash_attention_2\")\n",
        "if USE_FLASH:\n",
        "    try:\n",
        "        # Only attempt; if it fails, fall back silently\n",
        "        pip_install([\"flash-attn>=2.5.6.post1\"])\n",
        "    except Exception as e:\n",
        "        print(\"[SETUP] flash-attn install failed; falling back to SDPA:\", e)\n",
        "        ATTN_IMPL = \"sdpa\"\n",
        "\n",
        "# ---- Import torch *after* installs to avoid mixed-state imports\n",
        "import importlib, json, os, torch\n",
        "\n",
        "VERSIONS = {\n",
        "    \"python\": sys.version,\n",
        "    \"platform\": platform.platform(),\n",
        "    \"cuda_available\": torch.cuda.is_available(),\n",
        "    \"cuda_device\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
        "    \"torch\": getattr(importlib.import_module(\"torch\"), \"__version__\", None),\n",
        "    \"transformers\": getattr(importlib.import_module(\"transformers\"), \"__version__\", None),\n",
        "    \"peft\": getattr(importlib.import_module(\"peft\"), \"__version__\", None),\n",
        "    \"trl\": getattr(importlib.import_module(\"trl\"), \"__version__\", None),\n",
        "    \"accelerate\": getattr(importlib.import_module(\"accelerate\"), \"__version__\", None),\n",
        "    \"datasets\": getattr(importlib.import_module(\"datasets\"), \"__version__\", None),\n",
        "    \"bitsandbytes\": getattr(importlib.import_module(\"bitsandbytes\"), \"__version__\", None),\n",
        "}\n",
        "\n",
        "# Torch/compute flags\n",
        "try:\n",
        "    # Only set TF32 if GPU supports it; ignore undefined TF32 variable\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "VERSIONS[\"bf16_supported\"] = bool(bf16_ok)\n",
        "\n",
        "# Bitsandbytes sanity\n",
        "try:\n",
        "    from bitsandbytes.optim import PagedAdamW8bit as _T\n",
        "    VERSIONS[\"bnb_paged_adamw_8bit_ok\"] = True\n",
        "except Exception as e:\n",
        "    VERSIONS[\"bnb_paged_adamw_8bit_ok\"] = False\n",
        "    VERSIONS[\"bnb_error\"] = str(e)\n",
        "\n",
        "VERSIONS_JSON = os.path.join(FULL_DIR, \"VERSIONS.json\")\n",
        "with open(VERSIONS_JSON, \"w\") as f:\n",
        "    json.dump(VERSIONS, f, indent=2)\n",
        "print(\"[SETUP] Versions saved:\", VERSIONS_JSON)\n",
        "print(json.dumps(VERSIONS, indent=2))\n",
        "\n",
        "print(\"[SETUP] If needed, run: from huggingface_hub import login ; login()\")\n",
        "print(\"[SETUP] If in Colab, mount Drive: from google.colab import drive ; drive.mount('/content/drive')\")\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57ooCYLNvn3-"
      },
      "source": [
        "# Load Mixtral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNwtYDD9votW"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# [3/6] LOAD MIXTRAL — 4-bit NF4 QLoRA-ready + tokenizer\n",
        "# =========================\n",
        "import gc, torch, os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# ---- Model & runtime ----\n",
        "MODEL_NAME = f\"mistralai/{MODEL}-8x7B-Instruct-v0.1\"   # e.g., mistralai/Mixtral-8x7B-Instruct-v0.1\n",
        "ATTN_IMPL = \"auto\"                                      # \"flash_attention_2\" | \"sdpa\" | \"auto\"\n",
        "DTYPE_COMPUTE = \"bf16\"                                  # bf16|fp16|fp32\n",
        "MAX_SEQ_LEN = 2048\n",
        "TF32 = True\n",
        "\n",
        "# Honor TF32\n",
        "try:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = bool(TF32)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Clean up GPU memory before loading\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "\n",
        "# Map compute dtype robustly\n",
        "_compute_dtype_map = {\"bf16\": torch.bfloat16, \"fp16\": torch.float16, \"fp32\": torch.float32}\n",
        "_compute_dtype = _compute_dtype_map.get(str(DTYPE_COMPUTE).lower(), torch.bfloat16)\n",
        "\n",
        "# NF4 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=_compute_dtype,\n",
        ")\n",
        "\n",
        "# Allow more headroom for sequential fallback on 80GB\n",
        "SEQUENTIAL_FALLBACK_MAXMEM = {0: os.environ.get(\"MAXMEM0\", \"78GiB\")}\n",
        "\n",
        "# Load base model with fallback if OOM\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        low_cpu_mem_usage=True,\n",
        "        trust_remote_code=False,\n",
        "    )\n",
        "except RuntimeError as e:\n",
        "    if \"CUDA out of memory\" in str(e):\n",
        "        print(\"[load] OOM on auto; retrying with sequential low-peak placement…\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"sequential\",\n",
        "            max_memory=SEQUENTIAL_FALLBACK_MAXMEM,\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=False,\n",
        "        )\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# Attention kernel selection\n",
        "try:\n",
        "    if ATTN_IMPL == \"flash_attention_2\":\n",
        "        model.config.attn_implementation = \"flash_attention_2\"\n",
        "    elif ATTN_IMPL == \"sdpa\":\n",
        "        model.config.attn_implementation = \"sdpa\"\n",
        "    else:\n",
        "        try:\n",
        "            model.config.attn_implementation = \"flash_attention_2\"\n",
        "        except Exception:\n",
        "            model.config.attn_implementation = \"sdpa\"\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Gradient checkpointing implies use_cache=False during training\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Tokenizer\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "tok.padding_side = \"right\"\n",
        "tok.truncation_side = \"right\"\n",
        "try:\n",
        "    tok.model_max_length = int(MAX_SEQ_LEN)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Align pad/eos\n",
        "model.config.pad_token_id = tok.pad_token_id\n",
        "try:\n",
        "    model.generation_config.pad_token_id = tok.pad_token_id\n",
        "except Exception:\n",
        "    pass\n",
        "if getattr(model.generation_config, \"eos_token_id\", None) is None:\n",
        "    try:\n",
        "        model.generation_config.eos_token_id = tok.eos_token_id\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Ensure tensor-parallel setting won't confuse PEFT\n",
        "try:\n",
        "    model.config.pretraining_tp = 1\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Quick MoE param discovery (for later targeting)\n",
        "moe_params = [n for n, _ in model.named_parameters() if \"block_sparse_moe.experts\" in n]\n",
        "print(f\"[load] MoE params detected: {len(moe_params)} (sample)\")\n",
        "for n in moe_params[:8]:\n",
        "    print(\"  \", n)\n",
        "assert any((\".w1.weight\" in n) or (\".w2.weight\" in n) or (\".w3.weight\" in n) for n in moe_params), \\\n",
        "    \"Did not find experts.*.(w1|w2|w3); check model param names.\"\n",
        "\n",
        "# Also surface likely router/gate param names for later freezing/regularization\n",
        "router_like = [n for n, _ in model.named_parameters() if (\"gate\" in n or \"router\" in n) and \"experts\" not in n]\n",
        "print(f\"[load] Router-like params (sample): {router_like[:4]}\")\n",
        "\n",
        "print(\"[load] First param device:\", next(model.parameters()).device)\n",
        "print(\"[load] Compute dtype:\", _compute_dtype)\n",
        "print(\"[load] Attn impl:\", getattr(model.config, \"attn_implementation\", \"default\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClFnweObv6c-"
      },
      "source": [
        "# Load LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SKo5BTZiB5s0"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# [4/6] LOAD LORA — rs-LoRA, MoE-only by default, layer/expert scoped\n",
        "# =========================\n",
        "import os, re, torch\n",
        "from collections import Counter\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "\n",
        "# ---------- config compatibility ----------\n",
        "# Prefer ROUTER_MODE if present; fall back to FREEZE_ROUTER bool.\n",
        "if \"ROUTER_MODE\" in globals():\n",
        "    _ROUTER_MODE = str(ROUTER_MODE).lower()\n",
        "elif \"FREEZE_ROUTER\" in globals():\n",
        "    _ROUTER_MODE = \"frozen\" if bool(FREEZE_ROUTER) else \"free\"\n",
        "else:\n",
        "    _ROUTER_MODE = \"frozen\"\n",
        "\n",
        "# Required globals expected (with reasonable fallbacks)\n",
        "LORA_R         = int(globals().get(\"LORA_R\", 8))\n",
        "LORA_ALPHA     = int(globals().get(\"LORA_ALPHA\", 32))\n",
        "LORA_DROPOUT   = float(globals().get(\"LORA_DROPOUT\", 0.0))\n",
        "BIAS           = str(globals().get(\"BIAS\", \"none\"))\n",
        "USE_RS_LORA    = bool(globals().get(\"USE_RS_LORA\", True))\n",
        "GRADIENT_CHECKPOINTING = bool(globals().get(\"GRADIENT_CHECKPOINTING\", True))\n",
        "GC_REENTRANT   = bool(globals().get(\"GC_REENTRANT\", False))\n",
        "FREEZE_NON_TARGET = bool(globals().get(\"FREEZE_NON_TARGET\", True))\n",
        "\n",
        "assert \"model\" in globals(), \"[LoRA] Expected a loaded `model` in globals.\"\n",
        "assert \"tok\"   in globals(), \"[LoRA] Expected a tokenizer callable `tok` in globals.\"\n",
        "\n",
        "# Sanity on TARGET_PARAMS (supports MoE: w1/w2/w3, and optionally attn: q_proj/k_proj/v_proj/o_proj)\n",
        "assert \"TARGET_PARAMS\" in globals(), \"[LoRA] Provide TARGET_PARAMS = ['w1','w2','w3'] (and/or attn proj names).\"\n",
        "assert isinstance(TARGET_PARAMS, (tuple, list)), \"TARGET_PARAMS must be a tuple/list of module names\"\n",
        "_EXPERT_PARAM_SET = {\"w1\",\"w2\",\"w3\"}\n",
        "_ATTN_PARAM_SET   = {\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"}\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _dtype_hist(m):\n",
        "    from collections import Counter as _C\n",
        "    return dict(_C(str(p.dtype) for _, p in m.named_parameters()))\n",
        "def _device_hist(m):\n",
        "    from collections import Counter as _C\n",
        "    return dict(_C(str(p.device) for _, p in m.named_parameters()))\n",
        "\n",
        "def _discover_num_experts():\n",
        "    # find any layer's experts.{e}.w1 and count e indices (prefix-agnostic)\n",
        "    pat = re.compile(r\"\\.layers\\.(\\d+)\\.block_sparse_moe\\.experts\\.(\\d+)\\.w1(\\.|$)\")\n",
        "    seen = set()\n",
        "    for n, _ in model.named_parameters():\n",
        "        m = pat.search(n)\n",
        "        if m:\n",
        "            seen.add(int(m.group(2)))\n",
        "    return max(seen)+1 if seen else 8  # default Mixtral\n",
        "\n",
        "NUM_EXPERTS = _discover_num_experts()\n",
        "\n",
        "def _expand_layers(L):\n",
        "    if L == \"all\":\n",
        "        return tuple(range(getattr(model.config, \"num_hidden_layers\", 32)))\n",
        "    return tuple(sorted(set(L)))\n",
        "\n",
        "def _expand_experts(E):\n",
        "    if E == \"all\":\n",
        "        return tuple(range(NUM_EXPERTS))\n",
        "    return tuple(sorted(set(E)))\n",
        "\n",
        "# ---------- expand targeting ----------\n",
        "assert \"L\" in globals(), \"[LoRA] Provide L (list of layer idx or 'all').\"\n",
        "assert \"E\" in globals(), \"[LoRA] Provide E (list of expert idx or 'all').\"\n",
        "TARGET_LAYERS  = _expand_layers(L)\n",
        "TARGET_EXPERTS = _expand_experts(E)\n",
        "TARGET_PARAMS  = tuple(sorted(set(TARGET_PARAMS)))\n",
        "EXPERT_PARAMS  = tuple(sorted(_EXPERT_PARAM_SET.intersection(TARGET_PARAMS)))\n",
        "ATTN_PARAMS    = tuple(sorted(_ATTN_PARAM_SET.intersection(TARGET_PARAMS)))\n",
        "\n",
        "# ---------- Build ALLOWLIST of *true* suffixes (to use with .endswith) ----------\n",
        "#  • MoE MLP: layers.{L}.block_sparse_moe.experts.{E}.w{1,2,3}\n",
        "#  • Attention: layers.{L}.self_attn.{q_proj,k_proj,v_proj,o_proj}\n",
        "allow_suffixes = set()\n",
        "for L_i in TARGET_LAYERS:\n",
        "    for P in ATTN_PARAMS:\n",
        "        allow_suffixes.add(f\"layers.{L_i}.self_attn.{P}\")\n",
        "    for E_i in TARGET_EXPERTS:\n",
        "        for P in EXPERT_PARAMS:\n",
        "            allow_suffixes.add(f\"layers.{L_i}.block_sparse_moe.experts.{E_i}.{P}\")\n",
        "\n",
        "if not allow_suffixes:\n",
        "    raise AssertionError(\"[LoRA] Empty allowlist — check TARGET_PARAMS / E / L.\")\n",
        "\n",
        "# ---------- prep model for k-bit training ----------\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# ---------- gradient checkpointing & input grads ----------\n",
        "if GRADIENT_CHECKPOINTING:\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": GC_REENTRANT})\n",
        "    except TypeError:\n",
        "        model.gradient_checkpointing_enable()\n",
        "    if hasattr(model, \"enable_input_require_grads\"):\n",
        "        model.enable_input_require_grads()\n",
        "\n",
        "# ---------- router trainability ----------\n",
        "def _is_router_name(name: str) -> bool:\n",
        "    # anything that looks like routing/gating but not the expert weights themselves\n",
        "    return (\"block_sparse_moe\" in name) and ((\"gate\" in name) or (\".router.\" in name)) and (\".experts.\" not in name)\n",
        "\n",
        "for n, p in model.named_parameters():\n",
        "    if _is_router_name(n):\n",
        "        p.requires_grad = (_ROUTER_MODE in (\"free\",\"safemoe\"))\n",
        "\n",
        "_n_gate_train = sum(1 for n,p in model.named_parameters() if p.requires_grad and _is_router_name(n))\n",
        "print(f\"[router] mode={_ROUTER_MODE} | trainable router params: {_n_gate_train}\")\n",
        "\n",
        "# ---------- LoRA config ----------\n",
        "# target_modules tells PEFT which MODULE NAMES to wrap globally; we later enable only our allowlist\n",
        "target_modules_for_peft = sorted(set(list(EXPERT_PARAMS) + list(ATTN_PARAMS)))\n",
        "if not target_modules_for_peft:\n",
        "    # default to MoE-only if user passed only unknowns\n",
        "    target_modules_for_peft = [\"w1\",\"w2\",\"w3\"]\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=int(LORA_R),\n",
        "    lora_alpha=int(LORA_ALPHA),\n",
        "    lora_dropout=float(LORA_DROPOUT),\n",
        "    bias=str(BIAS),\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=target_modules_for_peft,\n",
        "    use_rslora=bool(USE_RS_LORA),\n",
        ")\n",
        "\n",
        "# ---------- inject adapters ----------\n",
        "if hasattr(model, \"peft_config\"):\n",
        "    raise RuntimeError(\"[LoRA] Adapters already attached; reload base model (Step 3) before re-attaching.\")\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "# ---------- restrict adapters to allowlist ----------\n",
        "adapted_total = adapted_enabled = adapted_disabled = 0\n",
        "matched_names, disabled_names = [], []\n",
        "\n",
        "def _is_lora_augmented(mod):\n",
        "    return hasattr(mod, \"lora_A\") and hasattr(mod, \"lora_B\")\n",
        "\n",
        "for mod_name, mod in model.named_modules():\n",
        "    if not _is_lora_augmented(mod):\n",
        "        continue\n",
        "    adapted_total += 1\n",
        "    enable = any(mod_name.endswith(sfx) for sfx in allow_suffixes)\n",
        "    # Set requires_grad on LoRA A/B only\n",
        "    pA = getattr(mod, \"lora_A\", None)\n",
        "    pB = getattr(mod, \"lora_B\", None)\n",
        "    if hasattr(pA, \"default\"): pA = pA.default\n",
        "    if hasattr(pB, \"default\"): pB = pB.default\n",
        "    if enable:\n",
        "        matched_names.append(mod_name)\n",
        "        for _, t in pA.named_parameters(recurse=False): t.requires_grad = True\n",
        "        for _, t in pB.named_parameters(recurse=False): t.requires_grad = True\n",
        "        adapted_enabled += 1\n",
        "    else:\n",
        "        disabled_names.append(mod_name)\n",
        "        for _, t in pA.named_parameters(recurse=False): t.requires_grad = False\n",
        "        for _, t in pB.named_parameters(recurse=False): t.requires_grad = False\n",
        "        adapted_disabled += 1\n",
        "\n",
        "# ---------- freeze everything else except router (if router is trainable) ----------\n",
        "if FREEZE_NON_TARGET:\n",
        "    for n, p in model.named_parameters():\n",
        "        if \"lora_\" in n:\n",
        "            continue\n",
        "        if _is_router_name(n) and (_ROUTER_MODE in (\"free\",\"safemoe\")):\n",
        "            continue  # leave router trainable per mode\n",
        "        p.requires_grad = False\n",
        "\n",
        "# ---------- hardening: verify expected modules are actually wrapped ----------\n",
        "wrapped_names = [n for n, m in model.named_modules() if _is_lora_augmented(m)]\n",
        "missing_wrapped = [\n",
        "    f\"...{sfx}\" for sfx in sorted(allow_suffixes)\n",
        "    if not any(n.endswith(sfx) for n in wrapped_names)\n",
        "]\n",
        "if missing_wrapped:\n",
        "    print(\"[LoRA][WARN] Some expected modules did not receive LoRA wrappers (first 20 shown):\")\n",
        "    for s in missing_wrapped[:20]:\n",
        "        print(\"  •\", s)\n",
        "\n",
        "# ---------- inventories ----------\n",
        "def _is_expert_ffn_module(name: str) -> bool:\n",
        "    return (\".block_sparse_moe.experts.\" in name) and name.endswith((\".w1\", \".w2\", \".w3\"))\n",
        "all_ffn_modules = [n for n, m in model.named_modules() if _is_expert_ffn_module(n)]\n",
        "enabled_ffn_modules = matched_names\n",
        "\n",
        "# ---------- assertions & diagnostics ----------\n",
        "if adapted_enabled == 0:\n",
        "    print(\"[LoRA][DEBUG] No enabled adapters matched. Nearby LoRA-wrapped names around first target layer:\")\n",
        "    hint_layer = TARGET_LAYERS[0]\n",
        "    modules_dict = dict(model.named_modules())\n",
        "    near = []\n",
        "    for name, mod in modules_dict.items():\n",
        "        is_wrapped = _is_lora_augmented(mod) or (\"lora_\" in name)\n",
        "        if is_wrapped and (f\".layers.{hint_layer}.\" in name):\n",
        "            near.append(name)\n",
        "    for n in near[:20]:\n",
        "        print(\"  •\", n)\n",
        "    print(\"[LoRA][DEBUG] Expected suffixes (match via .endswith):\")\n",
        "    for s in sorted(allow_suffixes):\n",
        "        print(\"  →\", s)\n",
        "    raise AssertionError(\"[LoRA] No allowed modules received trainable adapters. Check E/L/TARGET_PARAMS.\")\n",
        "\n",
        "_total_params     = sum(p.numel() for p in model.parameters())\n",
        "_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "_pct = 100.0 * _trainable_params / _total_params\n",
        "\n",
        "print(\"=== LoRA attach summary ===\")\n",
        "print(f\"use_rslora={getattr(lora_cfg,'use_rslora',False)} | r={LORA_R} | alpha={LORA_ALPHA} | dropout={LORA_DROPOUT}\")\n",
        "print(f\"Layers={tuple(TARGET_LAYERS)} | Experts={tuple(TARGET_EXPERTS)} | Params={tuple(TARGET_PARAMS)}\")\n",
        "print(f\"Router mode={_ROUTER_MODE} | FREEZE_NON_TARGET={FREEZE_NON_TARGET}\")\n",
        "print(f\"Wrapped LoRA modules: {adapted_total} | enabled: {adapted_enabled} | disabled: {adapted_disabled}\")\n",
        "print(f\"Expert FFN modules: {len(all_ffn_modules)} | with trainable adapters: {len(enabled_ffn_modules)}\")\n",
        "print(f\"Trainable params: {_trainable_params:,} / {_total_params:,} ({_pct:.4f}%)\")\n",
        "print(\"dtype histogram:\", _dtype_hist(model))\n",
        "print(\"device histogram:\", _device_hist(model))\n",
        "\n",
        "# Persist manifest of exactly-enabled modules (repro)\n",
        "FULL_DIR = globals().get(\"FULL_DIR\", \"./run_artifacts\")\n",
        "os.makedirs(os.path.join(FULL_DIR, \"logs\"), exist_ok=True)\n",
        "with open(os.path.join(FULL_DIR, \"logs\", \"enabled_lora_modules.txt\"), \"w\") as f:\n",
        "    f.write(\"\\n\".join(sorted(enabled_ffn_modules)))\n",
        "\n",
        "# ---------- dry run ----------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    device = next(model.parameters()).device\n",
        "    _ = model(**tok(\"LoRA dry run.\", return_tensors=\"pt\").to(device))\n",
        "print(\"[LoRA] Forward dry-run OK\")\n",
        "model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WESIf97NFKIV"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Verifies: adapters attached, allowlist matches, trainables > 0,\n",
        "# router mode/params, coverage over (L,E), dtype/devices, and a 1-step grad test.\n",
        "# ================================\n",
        "import re, math, torch\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "assert \"model\" in globals(), \"[SANITY] `model` not found.\"\n",
        "assert \"tok\"   in globals(), \"[SANITY] `tok` tokenizer callable not found.\"\n",
        "assert \"allow_suffixes\" in globals(), \"[SANITY] `allow_suffixes` not found (build your allowlist first).\"\n",
        "assert \"TARGET_PARAMS\" in globals(), \"[SANITY] `TARGET_PARAMS` missing.\"\n",
        "assert \"L\" in globals() and \"E\" in globals(), \"[SANITY] Provide L (layers) and E (experts).\"\n",
        "_ROUTER_MODE = str(globals().get(\"ROUTER_MODE\", \"frozen\")).lower()\n",
        "FREEZE_NON_TARGET = bool(globals().get(\"FREEZE_NON_TARGET\", True))\n",
        "\n",
        "def _is_lora_augmented(m):\n",
        "    return hasattr(m, \"lora_A\") and hasattr(m, \"lora_B\")\n",
        "\n",
        "def _is_router_name(name: str) -> bool:\n",
        "    return (\"block_sparse_moe\" in name) and ((\"gate\" in name) or (\".router.\" in name)) and (\".experts.\" not in name)\n",
        "\n",
        "def _dtype_hist(m): return dict(Counter(str(p.dtype) for _, p in m.named_parameters()))\n",
        "def _device_hist(m): return dict(Counter(str(p.device) for _, p in m.named_parameters()))\n",
        "\n",
        "# ---- 1) Adapters present & wrapped modules overview\n",
        "wrapped_modules = [n for n, m in model.named_modules() if _is_lora_augmented(m)]\n",
        "print(f\"[SANITY] LoRA-wrapped modules: {len(wrapped_modules)}\")\n",
        "print(\"         (showing up to 10)\")\n",
        "for n in wrapped_modules[:10]:\n",
        "    print(\"  •\", n)\n",
        "\n",
        "# ---- 2) Allowlist overlap (what will actually be trainable)\n",
        "matched_wrapped = [n for n in wrapped_modules if any(n.endswith(sfx) for sfx in allow_suffixes)]\n",
        "print(f\"[SANITY] Allowlist suffixes: {len(allow_suffixes)}\")\n",
        "print(f\"[SANITY] Wrapped modules matching allowlist: {len(matched_wrapped)}\")\n",
        "for n in matched_wrapped[:10]:\n",
        "    print(\"  ✓\", n)\n",
        "\n",
        "# ---- 3) Count trainable params & list a few tensors\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "pct = 100.0 * trainable_params / max(1, total_params)\n",
        "print(f\"[SANITY] Trainable params: {trainable_params:,} / {total_params:,} ({pct:.4f}%)\")\n",
        "\n",
        "trainable_tensors = [n for n,p in model.named_parameters() if p.requires_grad]\n",
        "print(f\"[SANITY] Trainable tensors: {len(trainable_tensors)} (showing up to 15)\")\n",
        "for n in trainable_tensors[:15]:\n",
        "    print(\"  •\", n)\n",
        "\n",
        "# ---- 4) Router mode & trainable router params\n",
        "router_trainables = [n for n,p in model.named_parameters() if p.requires_grad and _is_router_name(n)]\n",
        "print(f\"[SANITY] Router mode={_ROUTER_MODE} | trainable router tensors: {len(router_trainables)}\")\n",
        "if router_trainables[:5]:\n",
        "    print(\"         (first few)\")\n",
        "    for n in router_trainables[:5]:\n",
        "        print(\"  •\", n)\n",
        "\n",
        "# ---- 5) Coverage matrix over (Layer, Expert) for MoE w1/w2/w3\n",
        "# Extract (L,E,proj) from matched module names\n",
        "coverage = defaultdict(lambda: defaultdict(set))  # coverage[L][E] -> set of {'w1','w2','w3'}\n",
        "pat = re.compile(r\"\\.layers\\.(\\d+)\\.block_sparse_moe\\.experts\\.(\\d+)\\.(w[123])$\")\n",
        "for n in matched_wrapped:\n",
        "    m = pat.search(n)\n",
        "    if m:\n",
        "        L_i = int(m.group(1)); E_i = int(m.group(2)); P = m.group(3)\n",
        "        coverage[L_i][E_i].add(P)\n",
        "\n",
        "# summarize desired vs actual\n",
        "def _expand_layers(L):\n",
        "    if L == \"all\":\n",
        "        return tuple(range(getattr(model.config, \"num_hidden_layers\", 32)))\n",
        "    return tuple(sorted(set(L)))\n",
        "\n",
        "def _expand_experts(E):\n",
        "    # detect count by scanning (fallback=8)\n",
        "    seen = set()\n",
        "    pat_e = re.compile(r\"\\.layers\\.\\d+\\.block_sparse_moe\\.experts\\.(\\d+)\\.w1\")\n",
        "    for n,_ in model.named_parameters():\n",
        "        m = pat_e.search(n)\n",
        "        if m: seen.add(int(m.group(1)))\n",
        "    NUM_E = max(seen)+1 if seen else 8\n",
        "    if E == \"all\":\n",
        "        return tuple(range(NUM_E))\n",
        "    return tuple(sorted(set(E)))\n",
        "\n",
        "LAYERS  = _expand_layers(L)\n",
        "EXPERTS = _expand_experts(E)\n",
        "want_params = set(TARGET_PARAMS)\n",
        "\n",
        "missing = []\n",
        "hit = 0\n",
        "for Li in LAYERS:\n",
        "    for Ei in EXPERTS:\n",
        "        got = coverage.get(Li, {}).get(Ei, set())\n",
        "        need = want_params.intersection({\"w1\",\"w2\",\"w3\"})\n",
        "        if need:\n",
        "            if not need.issubset(got):\n",
        "                missing.append((Li, Ei, sorted(list(need - got))))\n",
        "            else:\n",
        "                hit += 1\n",
        "\n",
        "print(f\"[SANITY] MoE coverage: hit {hit} (L,E pairs have all desired w1/w2/w3). Missing: {len(missing)}\")\n",
        "for (Li,Ei,miss) in missing[:10]:\n",
        "    print(f\"  • Missing {miss} at L={Li}, E={Ei}\")\n",
        "\n",
        "# ---- 6) Dtype / device overview\n",
        "print(\"[SANITY] dtype histogram:\", _dtype_hist(model))\n",
        "print(\"[SANITY] device histogram:\", _device_hist(model))\n",
        "\n",
        "# ---- 7) One-step gradient flow check (very small prompt)\n",
        "# Ensures LoRA tensors receive non-zero grads; base weights remain frozen (if FREEZE_NON_TARGET=True).\n",
        "model.train()\n",
        "model.zero_grad(set_to_none=True)\n",
        "sample = \"Sanity check: the quick brown fox.\"\n",
        "inputs = tok(sample, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
        "# Use standard LM loss: labels = input_ids (shift handled by model)\n",
        "out = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "loss = out.loss\n",
        "loss.backward()\n",
        "\n",
        "# LoRA grad stats\n",
        "lora_grads = []\n",
        "base_grads = []\n",
        "for n,p in model.named_parameters():\n",
        "    is_lora = (\"lora_\" in n)\n",
        "    if p.grad is None:\n",
        "        continue\n",
        "    gnorm = p.grad.detach().norm().item()\n",
        "    if is_lora and p.requires_grad:\n",
        "        lora_grads.append(gnorm)\n",
        "    if (not is_lora) and p.requires_grad and FREEZE_NON_TARGET:\n",
        "        base_grads.append(gnorm)\n",
        "\n",
        "print(f\"[SANITY] Loss={loss.item():.4f} | LoRA grad tensors (nonzero): {sum(g>0 for g in lora_grads)} / {len(lora_grads)}\")\n",
        "if FREEZE_NON_TARGET:\n",
        "    print(f\"[SANITY] Base (non-LoRA) grad tensors (should be ~0): {len(base_grads)}\")\n",
        "\n",
        "# Extra assertions to catch silent misconfig\n",
        "assert len(wrapped_modules) > 0, \"[SANITY] No LoRA-wrapped modules on model.\"\n",
        "assert len(matched_wrapped) > 0, \"[SANITY] Allowlist did not match any wrapped module names.\"\n",
        "assert trainable_params > 0,     \"[SANITY] 0 trainable params — check allowlist/L/E/TARGET_PARAMS.\"\n",
        "assert any(g > 0 for g in lora_grads), \"[SANITY] No non-zero grads on LoRA tensors after backward().\"\n",
        "print(\"[SANITY] All checks passed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J14UUUpEwacK"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geBkPzrUwbHU"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# [5/6] LOAD DATA — assistant-only masking, robust & collator-friendly\n",
        "# =========================\n",
        "import os, random, json\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ---------- Logging helpers (output-only; no functional change) ----------\n",
        "def _fmt_int(x):\n",
        "    try:\n",
        "        return f\"{int(x):,}\"\n",
        "    except Exception:\n",
        "        return str(x)\n",
        "\n",
        "def _h(s):\n",
        "    print(f\"\\n=== {s} ===\")\n",
        "\n",
        "def _log(k, v):\n",
        "    print(f\"{k:<18}: {v}\")\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "MODEL_NAME     = globals().get(\"MODEL_NAME\", MODEL_NAME)\n",
        "DATA_PATH      = globals().get(\"DATA_PATH\", DATA_PATH)\n",
        "MAX_SEQ_LEN    = int(globals().get(\"MAX_SEQ_LEN\", 2048))\n",
        "MAX_CHARS      = int(globals().get(\"MAX_CHARS\", 6000))\n",
        "ASSIST_ONLY    = bool(globals().get(\"ASSISTANT_ONLY_MASKING\", True))\n",
        "VAL_FRAC       = float(globals().get(\"VAL_FRAC\", 0.1))  # default 10% val split\n",
        "\n",
        "SEED = int(globals().get(\"SEEDS\", [globals().get(\"SEED\", 42)])[0]) if \"SEEDS\" in globals() else int(globals().get(\"SEED\", 42))\n",
        "random.seed(SEED)\n",
        "\n",
        "# tokenizer (reuse or create)\n",
        "if tok is None:\n",
        "    tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "\n",
        "tok.padding_side = \"right\"\n",
        "tok.truncation_side = \"right\"\n",
        "\n",
        "_h(\"Tokenizer\")\n",
        "_log(\"model\", MODEL_NAME)\n",
        "_log(\"max_len\", MAX_SEQ_LEN)\n",
        "_log(\"pad_id\", tok.pad_token_id)\n",
        "_log(\"eos_id\", tok.eos_token_id)\n",
        "\n",
        "# ---- Load raw JSONL ----\n",
        "raw = load_dataset(\"json\", data_files={\"train\": DATA_PATH})[\"train\"]\n",
        "\n",
        "_h(\"Raw dataset\")\n",
        "_log(\"path\", DATA_PATH)\n",
        "_log(\"rows\", _fmt_int(len(raw)))\n",
        "_log(\"columns\", raw.column_names)\n",
        "\n",
        "# ---- Basic schema guard ----\n",
        "def _schema_ok(ex):\n",
        "    return (\n",
        "        \"messages\" in ex\n",
        "        and isinstance(ex[\"messages\"], list)\n",
        "        and all(isinstance(m, dict) and \"role\" in m and \"content\" in m for m in ex[\"messages\"])\n",
        "    )\n",
        "\n",
        "viol = sum(1 for ex in raw if not _schema_ok(ex))\n",
        "_log(\"schema violations\", _fmt_int(viol))\n",
        "\n",
        "# ---- Render last assistant supervision (prompt/completion) ----\n",
        "def render_last_assistant(example):\n",
        "    msgs = example[\"messages\"]\n",
        "    last_ass_idx = max((i for i, m in enumerate(msgs) if m.get(\"role\") == \"assistant\"), default=None)\n",
        "    if last_ass_idx is None:\n",
        "        return {\"drop\": True}\n",
        "\n",
        "    prompt_msgs = msgs[:last_ass_idx]\n",
        "    assistant_turn = msgs[last_ass_idx]\n",
        "\n",
        "    # Full text includes target; prompt_only is everything before\n",
        "    full_txt = tok.apply_chat_template(prompt_msgs + [assistant_turn], tokenize=False, add_generation_prompt=False)\n",
        "    prompt_only = tok.apply_chat_template(prompt_msgs, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # We do NOT pad here; just inspect lengths to filter/guard\n",
        "    full_ids   = tok(full_txt, add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "    prompt_ids = tok(prompt_only, add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt_only,\n",
        "        \"completion\": full_txt[len(prompt_only):],\n",
        "        \"text\": full_txt,\n",
        "        \"prefix_len\": len(prompt_ids),     # tokens belonging to the prompt\n",
        "        \"n_tokens\": len(full_ids),\n",
        "        \"char_len\": len(full_txt),\n",
        "        \"drop\": False,\n",
        "    }\n",
        "\n",
        "_h(\"Render + guards\")\n",
        "print(\"[render] applying chat template + extracting last assistant…\")\n",
        "rendered = raw.map(render_last_assistant, desc=\"[render]\")\n",
        "\n",
        "# ---- Filtering: drop invalid/too-long items ----\n",
        "rendered = rendered.filter(lambda ex: ex.get(\"drop\") is False)\n",
        "rendered = rendered.filter(lambda ex: ex[\"char_len\"] <= MAX_CHARS)\n",
        "rendered = rendered.filter(lambda ex: ex[\"n_tokens\"] <= MAX_SEQ_LEN)\n",
        "rendered = rendered.filter(lambda ex: 0 < ex[\"prefix_len\"] < ex[\"n_tokens\"])\n",
        "_log(\"kept after guards\", _fmt_int(len(rendered)))\n",
        "\n",
        "# ---- Train/val split (conversation-level) ----\n",
        "keep_cols = {\"prompt\", \"completion\", \"text\", \"prefix_len\", \"n_tokens\", \"char_len\"}\n",
        "rendered = rendered.remove_columns([c for c in rendered.column_names if c not in keep_cols])\n",
        "\n",
        "split = rendered.train_test_split(test_size=VAL_FRAC, seed=SEED)\n",
        "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "_h(\"Split\")\n",
        "_log(\"VAL_FRAC\", VAL_FRAC)\n",
        "_log(\"train\", _fmt_int(len(train_ds)))\n",
        "_log(\"val\", _fmt_int(len(val_ds)))\n",
        "\n",
        "# ---- Tokenize + assistant-only masking (no padding here) ----\n",
        "# We DO NOT pad in this step. Padding is deferred to the data collator at batch time.\n",
        "# Labels:\n",
        "#   - Positions < prefix_len are set to -100 (ignored)\n",
        "#   - We do NOT add padding here, so there are no pad tokens in labels at this stage.\n",
        "#   - The collator will pad input_ids/labels/attention_mask and must set padded label positions to -100.\n",
        "def tok_and_mask(batch):\n",
        "    texts = batch[\"text\"]\n",
        "    prefix_lens = batch[\"prefix_len\"]\n",
        "\n",
        "    enc = tok(\n",
        "        texts,\n",
        "        add_special_tokens=False,\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LEN,\n",
        "        padding=False,                 # <-- no pre-padding; collator will handle padding\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "\n",
        "    input_ids = enc[\"input_ids\"]\n",
        "    attention_mask = enc[\"attention_mask\"]\n",
        "\n",
        "    labels = []\n",
        "    for ids, pref in zip(input_ids, prefix_lens):\n",
        "        lab = ids.copy()\n",
        "        if ASSIST_ONLY:\n",
        "            # ignore all prompt tokens\n",
        "            mask_upto = min(pref, len(lab))\n",
        "            for k in range(mask_upto):\n",
        "                lab[k] = -100\n",
        "        # No padding here; any future padded positions will be -100 inside the collator\n",
        "        labels.append(lab)\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "cols_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "TRAIN_DS_TOKENIZED = train_ds.map(tok_and_mask, batched=True, remove_columns=train_ds.column_names)\n",
        "VAL_DS_TOKENIZED   = val_ds.map(tok_and_mask,   batched=True, remove_columns=val_ds.column_names)\n",
        "\n",
        "# Torch-format the required columns\n",
        "TRAIN_DS_TOKENIZED.set_format(type=\"torch\", columns=cols_keep)\n",
        "VAL_DS_TOKENIZED.set_format(type=\"torch\", columns=cols_keep)\n",
        "\n",
        "DATA_OBJECTS = {\n",
        "    \"train_ds\": TRAIN_DS_TOKENIZED,\n",
        "    \"val_ds\":   VAL_DS_TOKENIZED,\n",
        "}\n",
        "\n",
        "_h(\"Ready\")\n",
        "_log(\"ASSIST_ONLY\", ASSIST_ONLY)\n",
        "print(f\"[OK] TRAIN/VAL ready. Using {int((1-VAL_FRAC)*100)}/{int(VAL_FRAC*100)} train/val split.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhSf42DeyBhQ"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTma1YUDxMXE"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# [6/6] FINETUNE — robust Trainer (manual padding collator, eval-safe, resumable)\n",
        "# =========================\n",
        "import os, json, time, torch\n",
        "from typing import List, Dict, Any\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers.trainer_callback import TrainerCallback, TrainerControl, TrainerState\n",
        "from transformers.trainer_utils import set_seed\n",
        "\n",
        "# --------- Seeds / determinism ---------\n",
        "SEED = int(SEEDS[0]) if isinstance(SEEDS, (list, tuple)) and len(SEEDS) > 0 else int(globals().get(\"SEED\", 42))\n",
        "set_seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "\n",
        "# --------- Datasets ---------\n",
        "train_ds = DATA_OBJECTS[\"train_ds\"]\n",
        "val_ds   = DATA_OBJECTS[\"val_ds\"]\n",
        "\n",
        "# --------- Output dirs ---------\n",
        "os.makedirs(FULL_DIR, exist_ok=True)\n",
        "CKPT_DIR = os.path.join(FULL_DIR, \"checkpoints\")\n",
        "LOG_DIR  = os.path.join(FULL_DIR, \"logs\")\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "RUN_META_JSON = os.path.join(LOG_DIR, \"run_meta.json\")\n",
        "\n",
        "# --------- Router mode (for optional SafeMoE penalty) ---------\n",
        "_router_mode = globals().get(\"ROUTER_MODE\", \"frozen\")\n",
        "if \"FREEZE_ROUTER\" in globals():\n",
        "    _router_mode = \"frozen\" if bool(FREEZE_ROUTER) else _router_mode\n",
        "_safe_lambda = float(globals().get(\"SAFE_MOE_LAMBDA\", 1.0))\n",
        "\n",
        "# --------- Persist run meta ---------\n",
        "_run_meta = dict(\n",
        "    model_name=MODEL_NAME,\n",
        "    dt=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    seeds=list(SEEDS),\n",
        "    max_seq_len=int(MAX_SEQ_LEN),\n",
        "    lora=dict(\n",
        "        use_rs_lora=bool(USE_RS_LORA), r=int(LORA_R), alpha=int(LORA_ALPHA), dropout=float(LORA_DROPOUT), bias=str(BIAS),\n",
        "        target_layers=tuple(TARGET_LAYERS), target_experts=tuple(TARGET_EXPERTS), target_params=tuple(TARGET_PARAMS),\n",
        "        router_mode=_router_mode, safe_moe_lambda=_safe_lambda,\n",
        "    ),\n",
        "    optim=dict(name=OPTIM, weight_decay=float(WEIGHT_DECAY), lr=float(LR), scheduler=LR_SCHEDULER,\n",
        "               warmup_ratio=float(WARMUP_RATIO), grad_accum=int(GRAD_ACCUM), per_device_batch=int(PER_DEVICE_BATCH)),\n",
        "    logging=dict(log_every=int(LOG_EVERY), save_every=int(SAVE_EVERY), save_total_limit=int(SAVE_TOTAL_LIMIT),\n",
        "                 report_to=REPORT_TO),\n",
        "    data_stats=dict(train=len(train_ds), val=len(val_ds)),\n",
        "    run_tag=RUN_TAG, full_dir=FULL_DIR,\n",
        ")\n",
        "with open(RUN_META_JSON, \"w\") as f:\n",
        "    json.dump(_run_meta, f, indent=2)\n",
        "\n",
        "# --------- Persist trainable inventory ---------\n",
        "trainable_names = [n for n, p in model.named_parameters() if p.requires_grad]\n",
        "with open(os.path.join(LOG_DIR, \"trainable_params.txt\"), \"w\") as f:\n",
        "    f.write(\"\\n\".join(trainable_names))\n",
        "\n",
        "# --------- Optimizer guard (bnb paged adamw 8bit) ---------\n",
        "if OPTIM == \"paged_adamw_8bit\":\n",
        "    try:\n",
        "        from bitsandbytes.optim import PagedAdamW8bit as _T  # noqa: F401\n",
        "    except Exception as e:\n",
        "        print(f\"[optim] WARNING: BnB PagedAdamW8bit unavailable: {e}\\n         Falling back to 'adamw_torch'.\")\n",
        "        OPTIM = \"adamw_torch\"\n",
        "\n",
        "# --------- Model runtime flags ---------\n",
        "use_bf16 = (str(DTYPE_COMPUTE).lower() == \"bf16\") and torch.cuda.is_available()\n",
        "if hasattr(model.config, \"use_cache\"):\n",
        "    model.config.use_cache = False  # important for training\n",
        "\n",
        "# --------- Manual padding collator (fast, mismatch-proof) ---------\n",
        "def _to_list(x):\n",
        "    # Accept torch.Tensor or list; return Python list\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x.tolist()\n",
        "    return list(x)\n",
        "\n",
        "def clm_collator(examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "    pad_id = tok.pad_token_id\n",
        "    ignore_index = -100\n",
        "\n",
        "    input_ids = [ _to_list(ex[\"input_ids\"]) for ex in examples ]\n",
        "    attn_masks = [ _to_list(ex[\"attention_mask\"]) for ex in examples ]\n",
        "    labels     = [ _to_list(ex[\"labels\"]) for ex in examples ]\n",
        "\n",
        "    # Sanity: lengths must match per example pre-padding\n",
        "    for i, (ids, lab) in enumerate(zip(input_ids, labels)):\n",
        "        if len(ids) != len(lab):\n",
        "            # If this ever triggers, your dataset preprocessing diverged\n",
        "            raise ValueError(f\"Example {i}: len(input_ids)={len(ids)} != len(labels)={len(lab)}\")\n",
        "\n",
        "    max_len = max(len(ids) for ids in input_ids)\n",
        "\n",
        "    def pad(seq, L, val):\n",
        "        return seq + [val] * (L - len(seq))\n",
        "\n",
        "    # Pad everything to max_len\n",
        "    input_ids = [ pad(ids, max_len, pad_id) for ids in input_ids ]\n",
        "    attn_masks = [ pad(m,   max_len, 0)     for m   in attn_masks ]\n",
        "    # Important: labels use -100 for pad positions\n",
        "    labels = [ pad(lab, max_len, ignore_index) for lab in labels ]\n",
        "\n",
        "    batch = {\n",
        "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "        \"attention_mask\": torch.tensor(attn_masks, dtype=torch.long),\n",
        "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "    }\n",
        "    return batch\n",
        "\n",
        "# --------- Training args (eval-safe) ---------\n",
        "args = TrainingArguments(\n",
        "    output_dir=CKPT_DIR,\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH,\n",
        "    per_device_eval_batch_size=max(1, PER_DEVICE_BATCH),\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LR,\n",
        "    adam_beta1=0.9, adam_beta2=0.95,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    lr_scheduler_type=LR_SCHEDULER,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    max_steps=MAX_STEPS if MAX_STEPS else None,\n",
        "    num_train_epochs=NUM_EPOCHS if NUM_EPOCHS else 1,\n",
        "\n",
        "    logging_steps=LOG_EVERY,\n",
        "    save_steps=SAVE_EVERY,\n",
        "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=SAVE_EVERY,\n",
        "    prediction_loss_only=True,     # no logits kept → avoids big concats\n",
        "    eval_accumulation_steps=8,     # harmless here; helps if metrics later enabled\n",
        "\n",
        "    bf16=use_bf16, fp16=False,\n",
        "    gradient_checkpointing=GRADIENT_CHECKPOINTING,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": bool(GC_REENTRANT)},\n",
        "    max_grad_norm=CLIP_GRAD_NORM,\n",
        "\n",
        "    report_to=None if REPORT_TO == \"none\" else REPORT_TO,\n",
        "    optim=OPTIM,\n",
        "    ddp_find_unused_parameters=False,\n",
        "    remove_unused_columns=False,\n",
        "    save_safetensors=True,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# --------- Optional: clear-cache callback ---------\n",
        "class ClearCacheCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        try:\n",
        "            torch.cuda.empty_cache()\n",
        "        except Exception:\n",
        "            pass\n",
        "        return control\n",
        "\n",
        "# --------- Build Trainer ---------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=clm_collator,   # ← manual, mismatch-proof collator\n",
        "    tokenizer=tok,\n",
        "    compute_metrics=None,\n",
        "    callbacks=[ClearCacheCallback()],\n",
        ")\n",
        "\n",
        "# --------- SafeMoE penalty (router_mode == 'safemoe') ---------\n",
        "if _router_mode == \"safemoe\":\n",
        "    def _is_router_name(name: str) -> bool:\n",
        "        return (\"block_sparse_moe\" in name) and ((\"gate\" in name) or (\".router.\" in name)) and (\".experts.\" not in name)\n",
        "\n",
        "    router_pre = {k: v.detach().float().clone() for k, v in model.named_parameters() if _is_router_name(k)}\n",
        "    λ = _safe_lambda\n",
        "    orig_compute_loss = trainer.compute_loss\n",
        "\n",
        "    def compute_loss_with_router_penalty(model, inputs, return_outputs=False):\n",
        "        out = orig_compute_loss(model, inputs, return_outputs=True)\n",
        "        loss, outputs = out\n",
        "        pen = torch.tensor(0.0, device=loss.device)\n",
        "        for k, v in model.named_parameters():\n",
        "            if _is_router_name(k) and v.requires_grad and (k in router_pre) and (router_pre[k].shape == v.shape):\n",
        "                p_new = torch.nn.functional.log_softmax(v.float(), dim=-1)\n",
        "                p_old = torch.nn.functional.log_softmax(router_pre[k].to(v.device), dim=-1)\n",
        "                pen = pen + torch.nn.functional.kl_div(p_new, p_old, log_target=True, reduction=\"batchmean\")\n",
        "        loss = loss + λ * pen\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    trainer.compute_loss = compute_loss_with_router_penalty\n",
        "    print(f\"[router] SafeMoE penalty enabled (λ={λ})\")\n",
        "\n",
        "# --------- Resume (if checkpoints exist) ---------\n",
        "resume_arg = None\n",
        "if bool(globals().get(\"RESUME\", False)):\n",
        "    ckpts = sorted(\n",
        "        [p for p in os.listdir(CKPT_DIR) if p.startswith(\"checkpoint-\")],\n",
        "        key=lambda s: int(s.split(\"-\")[-1])\n",
        "    ) if os.path.isdir(CKPT_DIR) else []\n",
        "    if ckpts:\n",
        "        resume_arg = os.path.join(CKPT_DIR, ckpts[-1])\n",
        "        print(f\"[resume] Resuming from: {resume_arg}\")\n",
        "\n",
        "print(f\"[run] Checkpoints → {CKPT_DIR}\")\n",
        "print(\"[run] Starting training…\")\n",
        "trainer.train(resume_from_checkpoint=resume_arg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyB-2YjTLThJ"
      },
      "outputs": [],
      "source": [
        "import os, signal\n",
        "\n",
        "# Get the current process ID\n",
        "pid = os.getpid()\n",
        "\n",
        "# Kill it\n",
        "os.kill(pid, signal.SIGKILL)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "J14UUUpEwacK"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7448d31aa4240ebaca0fc9866ccb5ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_87b4387943da43b18a662d2e4f3cf9fc"
          }
        },
        "dbe7ac6388ab48ea84071bc835a1ed6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1faa31ac1ab244c7842b35886b6af1a5",
            "placeholder": "​",
            "style": "IPY_MODEL_e46c7578df8148f9a2e0d2c33bd4efa0",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d18b46c6d11b428caef65539e970b707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f71eb6ccbce84e9883a87b27b46b40ec",
            "placeholder": "​",
            "style": "IPY_MODEL_4fdb78c54d494661a34bbfc7a4343066",
            "value": ""
          }
        },
        "06a67432daad4adfb491071c97e90261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_7fcf6fa7f7724d10b46691f627c7caa7",
            "style": "IPY_MODEL_3e7f73b9bc724cf2b889094898a0bcf5",
            "value": true
          }
        },
        "c370a2b0c73c4391a5f8421c57a3de1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_db2486ea92fb46f18944b8ab7be6f2da",
            "style": "IPY_MODEL_b2dfbf3f892b4ac59d69d4c5399dde1c",
            "tooltip": ""
          }
        },
        "4ced3fa45c5442adb3141d4984f30c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1193d47c443544c8b87a5b0b55b580f0",
            "placeholder": "​",
            "style": "IPY_MODEL_c16b7634553a494eb07e8d8db78f95f9",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "87b4387943da43b18a662d2e4f3cf9fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "1faa31ac1ab244c7842b35886b6af1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e46c7578df8148f9a2e0d2c33bd4efa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f71eb6ccbce84e9883a87b27b46b40ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fdb78c54d494661a34bbfc7a4343066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fcf6fa7f7724d10b46691f627c7caa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e7f73b9bc724cf2b889094898a0bcf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db2486ea92fb46f18944b8ab7be6f2da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2dfbf3f892b4ac59d69d4c5399dde1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1193d47c443544c8b87a5b0b55b580f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c16b7634553a494eb07e8d8db78f95f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f522a8c04934e118da5adb6c3f4ca16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fbdbea8764445dca4fc7325ffa877c3",
            "placeholder": "​",
            "style": "IPY_MODEL_adfb62e911ea4d01bc235d80b2042268",
            "value": "Connecting..."
          }
        },
        "8fbdbea8764445dca4fc7325ffa877c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adfb62e911ea4d01bc235d80b2042268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}